
\documentclass{article} % For LaTeX2e
\usepackage{iclr2024_conference,times}
\usepackage{graphicx}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}


\title{InformatiCup 2024 Ctrl-Alt-Defeat}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Max Buchholz \And Lukas Müller \And Levi Otterbach \And Raphael Weber}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}

\end{abstract}

\section{Introduction}
The development of generative Machine Learning models made huge steps forward in the last years considering the comparison with human crafted artefacts. Large Language Models (LLM) enable the generation of texts in all kinds of different flavours and styles. Image generative models like Vision Transformer (ViT) already created fake images being reported as real by media. Other news showed artists winning art competitions with generated art. This creates an interest in detecting which texts or images are AI generated and which ones are real images or human-written texts.

Therefore researchers evaluated different approaches to detect fake images and texts. Some adversarial attacks are already known to be able to reduce the performance of other computer vision tasks, e.g. visual object detection, by artificially manipulating real images.

As this is a very topical issue the "Gesellschaft für Informatik" \cite{GI} organizes a competition called "InformatiCup2024" \cite{InformatiCup} where the participants try to manipulate a detectable source in a way that the detector does not recognize it as AI-generated anymore. This raises the questions, whether and how it is possible to manipulate generated artefacts (e.g. images and texts) to reduce the accuracy of detectors.

It is necessary to evaluate whether such techniques exist, because otherwise one could rely on detectors, which can easily be tricked. If easy efficient manipulation techniques exist, detectors need to be improved or better detectors need to be developed that are robust against these techniques. But this requires a good understanding how the attacks are performed.

Therefore we came up with the following research questions:


RQ1: How can generated images be manipulated to trick fake image detectors?


RQ2: How can generated texts be manipulated to trick fake text generators?


The report is structured as follows: After this introduction chapter 2 presents related work, especially text and image generative models as well as fake image and text detectors. Chapter 3 presents the methodology of our work followed by chapter 4 presenting the results of our experiments. Afterwards the results will be discussed in chapter 5 and chapter 6 will conclude this paper.

\section{Related work}

\subsection{Generative models}

The first image generative model we use is the \textbf{Stable Diffusion} model. It is developed by engineers and researchers from CompVis (\url{https://github.com/CompVis}), Stability AI (\url{https://stability.ai/}) and LAION (\url{https://laion.ai/}) \cite{patil2022stable}.

Diffusion models achieve state-of-the-art synthesis results on image data by decomposing the image formation process into sequential applications of denoising autoencoders. Due to its work directly in pixel space, they are very computational expensive. To lower the computational costs, the Stable Diffusion model (v1.5) is applied in a latent space of powerful pretrained autoencoders. This works without losing quality and flexibility. \cite{Rombach_2022_CVPR}

As a second image generative model we use \textbf{OpenDalle} from dataautogpt3 (\url{https://huggingface.co/dataautogpt3/OpenDalle}). OpenDalle, as it is based on the Dall-E model, works by modelling text and image tokens as a single data stream. This is achieved by training a transformer to autoregressively model those tokens into the resulting stream.

This is done in two steps:
 
\begin{enumerate}
	\item The images get compressed in a $32 \times 32$ grid of image tokens to reduce the context size of the transformer.
	\item BPE-encoded text tokens are concatenated with the image tokens to model a joint distribution over text and image tokens with the training of an autoregressive transformer.
\end{enumerate}

The exact procedure how DALL-E models work proposed by Ramesh et. al is explained in their paper. \cite{ramesh2021zeroshot}

We choose the \textbf{GPT2} model as one possibility to generate texts for testing our augmentation methods. The GPT model tries to counter the scarcity of labeled text data for machine learning.

\cite{radford2018improving} try to avoid this problem in a simple two step procedure:

\begin{enumerate}
	\item A generative pre-training of a language model on a varying corpus of unlabeled text.
	\item Discriminative fine-tuning on every specific task.
\end{enumerate}

To achieve effective transfer with minor changes to the model architecture, task-aware input is used during transformations. \cite{radford2018improving}

The \textbf{Falcon-RW-1B} model built by TII (\url{https://www.tii.ae/}) is the second text generative model we use to test our augmentation models. It is a decoder-only model trained on 350 billion tokens of the RefinedWeb data set. This data set leverages strict filtering and stringent deduplication to uplift the quality of web data. \cite{penedo2023refinedweb}

\subsection{Detector models}

\textbf{Winston AI} is a commercial AI text detector that starts at 18\$ a month. They have two primary methods for checking if a text is AI written. Firstly linguistic analysis analyses every text for characteristics, that are different between human- and AI-generated work. One example characteristic is the repetitiveness, another is Data Training, where the detector saw many AI written texts of one model and learned unique characteristics that this model uses which then can be detected. So this method is more generator-specific. \cite{WinstonAI}

Another tool to automatically detect generated text is \textbf{GLTR}. It applies different baseline statistical methods to detect generation artefacts. To do so, it needs access to a certain model, for which it can predict, whether the text is likely to be generated. The text is compared with the distribution of possible consecutive words given the semantic context. The tool calculates which word would have been predicted as next word at each token in the text and allows to colour which words are how likely to appear by the model at a given position and context. If a text is coloured completely green, a lot of words have a high chance, that they would have been predicted at this position by e.g. the currently hosted GPT2 117M model. Therefore, it is likely that a mostly green coloured text is generated by the GPT2 model. This approach can also be used for other models. \cite{gehrmann2019gltr}

A more recent approach is presented in \cite{zellers2020defending}. With the aim to investigate in the detection of AI generated fake news, the authors created a model called \textbf{Grover}, which is on the one hand able to generate fake news, but on the other hand also pretty reliably detects, whether a given text is AI-generated by itself. The study also elaborates on how to deal with the issue on generated fake news. Therefore, inspired by IT security research, they advocate for the release of such models to allow the development of effective countermeasures like detector models.

The \textbf{RoBERTa Base OpenAI Detector} is the output detector model of GPT-2 \cite{solaiman2019release}. It was obtained by fine-tuning a RoBERTa base model with the outputs of the GPT-2 model containing 1,5 billion parameters. The RoBERTa Base OpenAI Detector can be used to classify texts, on whether they are likely to be created by the GPT-2 model. It was published by OpenAI, when the weights of the GPT-2 model 1,5 Billion parameter version was released. The model is limited to english texts only and is openly available under the MIT license.

The \textbf{RADAR-Vicuna-7B} text detector is an adversarial-learning trained model introduced by TrustSafeAI. In the training stage a detector tried to differentiate between human and AI text while a paraphraser trained to make the text more human. The AI-generated text for learning is extracted by the LLM Vicuna-7B-v1.1 written by LMSYS Org \cite{hu2023radar}.

The \textbf{AIorNot} detector by Nahrawy on Hugging Face is a pretrained swin-tiny-patch4-window7-224 fine-tuned on the AI or Not dataset. Swin-Transformer by Microsoft is a new Vision Transformer which makes use of hierarchical windows. This reduces the self-attention to not-overlapping windows of other areas of the image. \cite{liu2021swin}

The \textbf{ai-image-det-resnet18} model is a community made, fine-tuned, resnet18 based, pretrained image detection model. For fine-tuning the author kgmann on Hugging Face used the AI or Not dataset. ResNet-18 is a convolutional neural network that is 18 layers deep. It got pretrained by over a million images from the ImageNet database. \cite{he2015deep}


\section{Method}
To test and evaluate the effect of different data augmentation techniques on the detection capabilities of fake image or text detectors, we created a pipeline of three components: The first component is a generator, which is a model capable of generating fake images or texts. The second component is a processor which augments the data through a specified method, after given a generated image or text. The last component is a detector, which suggests an input being either real or fake. This way we can compare it to the ground truth, to evaluate the capabilities of the detector. By comparing the results with augmented artefacts and with the generated ones without augmentation, we can evaluate the effectiveness of different AI detector-tricking techniques.

We used the aforementioned models for generating and detecting texts and images. As for image processors, one was already given in the description of the "InformatiCup2024" \cite{InformatiCup}, the image noising using the Gaussian noise. Image noise is the random variation of brightness or color information in images produced by the sensor and circuitry of a scanner or digital camera. We used gaussian noice, because it can occure due to natural sources and it is the most common noise that affects images naturally. \cite{mohammed2016study}

There are several types of noise that can affect images. Some of these noise models are: Gaussian noise, White noise, Fractal noise, Salt \& Pepper noise, Periodic noise, Quantization noise, Speckle noise, Poisson noise, Poisson-Gaussian noise, Structured noise, Gamma noise and Rayleigh noise \cite{gonzalez2009digital}. The three common types of image noise are: Gaussian noise, Salt \& Pepper noise, and Speckle noise \cite{al2010comparative}. That is why we used those three, as well as the Poisson noise.

As for text processors, we evaluated three different methods of augmenting generated texts. The first approach was to replace every white-space in a given text by two white-spaces. The second approach was to add some typos to the texts. This has been implemented based on the nlpaug library, which is capable of simulating certain typos as if they were made on a keyboard by a human. The typos include mistakes, like hitting a wrong key, which is close to the right one. This results in typos often created by humans. The last approach is a translator, which has been used to translate the given text into a certain language (in this case French) and re-translate it into english. As translators are not deterministic but aim to keep the semantic meaning, the resulting text should be semantically the same, but might have a different formulation.

For the ease of use and low time consumption we implemented a simple python command line interface, that makes it easy to define a pipeline consisting of a generator, processor and evaluator. It is also possible to enter multiple processors for one pipeline to stack and apply them together on the content.

The resulting pipelines, especially for images, were very resource intensive and could not be run efficiently on our own machines. We chose to use the free Google Colab resources, consisting of a NVIDIA Tesla T4 GPU that speeds up the pipelines up to 40x compared to our local execution time and allowed us to use the same environment for all experiments. 

\section{Results + Discussion}
In the following section we take a look on the results of the aforementioned experiment.


Fig. \ref{fig:stable_diffusion} shows how the different processors and their combinations effect an image generated by the stable diffusion generator. The bars are structured in packs of three where the most left one represents the nahrawy evaluator, the middle one is resnet18 and on the right is the umm-maybe evaluator respectively. The different colors represent the different outputs. Black symbolizes that the image before and after processing was detected as AI-generated. Grey shows that both are detected to be human made, whereas for red the process did the exact opposite of what it should do and fool the detector in a way that after processing it is seen as AI-generated whereas before as human made. Lastly the green bar does exactly what is desired, so it pictures AI-generated content, where the executed augmentation lead to the detector guessing it was human-made. \\
The data depicted in fig. \ref{fig:stable_diffusion} shows, that only the nahrawy evaluator fooled the detector the wrong way round, but it is also the only one with the output of 'human-human'. Compared to the Dall-E generator in fig. \ref{fig:dallE} the amount of right tricking is relatively low. There are also other differences as here only umm-maybe achieved 'human-ai' but also 'human-human'.
It is visible that the gaussian noise image processor as well as the salt-and-pepper one are the best processors. This can be seen as they solely achieve a great result as well as their combinations with other ones. It is also worth mentioning that these results are only achieved when using the nahrawy evaluator. Whereas resnet18, which can be seen in both figures, couldn't be tricked by any processor.

\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{images/stable-diffusion.png}
	\caption{Stable Diffusion with different processors and nahrawy, resnet18 and umm-maybe as evaluators}
	\label{fig:stable_diffusion}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{images/dalle.png}
	\caption{Dall-E with different processors and nahrawy, resnet18 and umm-maybe as evaluators}
	\label{fig:dallE}
\end{figure}


In fig. \ref{fig:gpt2} the text processors are plotted. The colors stay the same and the concept as well, where the left bar is using the radar evaluator and the right one the roberta base one. It is visible that the double-whitespace processor solely has little effect whereas in combination with the typo processor the result rises enormously. This is due to the typo processor since it alone also achieves great results. The radar evaluator only produces only black bars. 
Similar results can be seen in fig. \ref{fig:falcon} where radar just contains black bars and typo as well, as its combination with the roberta base evaluator achieves great results. The difference is that with the falcon generator and the roberta base evaluator in combination, some texts are detected as human made even before processing, which explains the red and grey parts. 

\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{images/gpt2.png}
	\caption{GPT2 with different processors and nahrawy, resnet18 and umm-maybe as evaluators}
	\label{fig:gpt2}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{images/falcon.png}
	\caption{Falcon with different processors and nahrawy, resnet18 and umm-maybe as evaluators}
	\label{fig:falcon}
\end{figure}


To summarize we can say, that the combination between generators and evaluators plays a huge role in the results. In case of the text generators the difference seems minor, but for example the roberta base evaluator detected around ten falcon generated texts as human made in the middle case of fig. \ref{fig:falcon} and after processing as well, which means that if that would not have been the case, we would have a perfect score here. 
This becomes more clear when comparing fig. \ref{fig:stable_diffusion} with fig \ref{fig:dallE} where the importance of the used generators becomes visible. The stable-diffusion generator produces images where it is difficult to fool a detector whereas when using the same processors and detectors, the images of dallE can be processed in a way to fool some detectors.


Another noticeable result is in the usage of different evaluators. In both cases, text and image, we had one evaluator respectively which could not be fooled with any method. These evaluators where so strict, that they would even classify a human made image provided by us as AI-generated.

\section{Conclusion}


\subsubsection*{Acknowledgments}


\bibliography{bibliography}
\bibliographystyle{iclr2024_conference}

\appendix
\section{Appendix}
You may include other additional sections here.

\end{document}
