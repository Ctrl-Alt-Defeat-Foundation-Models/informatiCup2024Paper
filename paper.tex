
\documentclass{article} % For LaTeX2e
\usepackage{iclr2024_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}


\title{InformatiCup 2024 Ctrl-Alt-Defeat}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Max Buchholz, Lukas MÃ¼ller, Levi Otterbach \& Raphael Weber}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}

\end{abstract}

\section{Introduction}
The development of generative Machine Learning models made huge steps forward in the last years, considering the comparison with human crafted artefacts. Large Language Models (LLM) enable the generation of texts in all kinds of different flavours and styles. Image generating models like Vision Transformer (ViT) already created fake images being reported as real by media. Other news showed artists winning art competitions with generated art. This creates an interest in detecting which texts or images are AI generated and which ones are real images or human-written texts.

Therefore researchers evaluated different approaches to detect fake images and texts. Some adversarial attacks are already known to be able to reduce the performance of other computer vision tasks, e. g. visual object detection, by artificially manipulating real images. This raises the questions, whether and how it is possible to manipulate generated artefacts (e.g. images and texts) to reduce the accuracy of detectors.


It is necessary to evaluate whether such techniques exist, because otherwise one could rely on detectors, which can easily be tricked. If easy efficient manipulation techniques exist, detectors need to be improved or better detectors need to be developed that are robust against these techniques. But this requires a good understanding how the attacks are performed.

Therefore we came up with the following research questions:


RQ1: How can generated images be manipulated to trick fake image detectors?


RQ2: How can generated texts be manipulated to trick fake text generators?


The report is structured as follows: After this introduction chapter 2 presents related work, especially text and image generating models as well as fake image and text detectors. Chapter 3 presents the methodology of our work followed by chapter 4 presenting the results of our experiments. Afterwards the results will be discussed in chapter 5 and chapter 6 will conclude this paper.

\section{Related work}

\subsection{Generating models}

\subsubsection{Image generating models}

\subsubsection{Text generating models}

\subsection{Detector models}

\subsubsection{Image detector models}


The \textbf{AIorNot} detector by Nahrawy on Hugging Face is a pretrained swin-tiny-patch4-window7-224 fine-tuned on the AI or Not dataset. Swin-Transformer by Microsoft is a new Vision Transformer which makes use of hierarchical windows. This reduces the self-attention to not-overlapping windows of other areas of the image \cite{liu2021swin}.

The \textbf{ai-image-det-resnet18} model is a community made, fine-tuned, resnet18 based, pretrained image detection model. For fine-tuning the author kgmann on Hugging Face used the AI or Not dataset. ResNet-18 is a convolutional neural network that is 18 layers deep. It got pretrained by over a million images from the ImageNet database. \cite{he2015deep}


\subsubsection{Text detector models}

The \textbf{RoBERTa Base OpenAI Detector} is the output detector model of GPT-2. It was obtained by fine-tuning a RoBERTa base model with the outputs of the GPT-2 model containing 1,5 Billion parameters. The RoBERTa Base OpenAI Detector can be used to classify texts, on whether they are likely to be created by the GPT-2 model. It was published by OpenAI, when the weights of the GPT-2 model 1,5 Billion parameter version was released.

The \textbf{RADAR-Vicuna-7B} text detector is an adversarial-learning trained model introduced by TrustSafeAI. In the training stage a detector tried to differentiate between human and AI text while a paraphraser trained to make the text more human.
The AI-generated text for learning is extracted by the LLM Vicuna-7B-v1.1 written by LMSYS Org \cite{hu2023radar}.


\section{Method}
To test and evaluate the effect of different data augmentation techniques on the detection capabilities of fake image or text detectors, we created a pipeline of three components: The first component is a generator, which is a model capable of generating fake images or texts. The second component is a processor which augments the data through a specified method, after given a generated image or text. The last component is a detector, which suggests an input being either real or fake. This way we can compare it to the ground truth, to evaluate the capabilities of the detector. By comparing the results with augmented artefacts and with the generated ones without augmentation, we can evaluate the effectiveness of different AI detector-tricking techniques.

We used the aforementioned models for generating and detecting texts and images. As for image processors, one was already given in the description of the "InformatiCup2024" \cite{InformatiCup}, the image noising using the Gaussian noise. Image noise is the random variation of brightness or color information in images produced by the sensor and circuitry of a scanner or digital camera \cite{mohammed2016study}. The reason why the Gaussian one, also called normal noise, is being used, is because it is being caused by natural sources and the most common noise that affects images naturally \cite{mohammed2016study}. \\
There are several types of noise that can affect images. Some of these noise models are: Gaussian noise, White noise, Fractal noise, Salt \& Pepper noise, Periodic noise, Quantization noise, Speckle noise, Poisson noise, Poisson-Gaussian noise, Structured noise, Gamma noise and Rayleigh noise \cite{gonzalez2009digital}. The three common types of image noise are: Gaussian noise, Salt \& Pepper noise, and Speckle noise \cite{al2010comparative}. That is why we used those three, as well as the Poisson noise.

\section{Results}

\section{Discussion}

\section{Conclusion}


\subsubsection*{Acknowledgments}


\bibliography{bibliography}
\bibliographystyle{iclr2024_conference}

\appendix
\section{Appendix}
You may include other additional sections here.

\end{document}
